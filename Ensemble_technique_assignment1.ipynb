{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqH3nCj32Rx3lU7u/1ldwN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riturajkumari/Ensemble_technique/blob/main/Ensemble_technique_assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. What is an ensemble technique in machine learning?**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "srVauU6E92EW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In machine learning, an ensemble technique is a method that combines multiple models to improve the overall performance of the model. The models may be the same or different types and may use the same or different training data. The predictions may be combined using statistics or more sophisticated methods. Ensemble models can be used for classification or regression problems.\n"
      ],
      "metadata": {
        "id": "BZUpLUWmAPJI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UJADDSz593xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. Why are ensemble techniques used in machine learning?**"
      ],
      "metadata": {
        "id": "dF7tJkQ-94Br"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two main reasons to use an ensemble over a single model, and they are related; they are:\n",
        "\n",
        "- Performance: An ensemble can make better predictions and achieve better performance than any single contributing model.\n",
        "- Robustness: An ensemble reduces the spread or dispersion of the predictions and model performance."
      ],
      "metadata": {
        "id": "8M4ihAv3Alw5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "skMhsyaM9-Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. What is bagging?**"
      ],
      "metadata": {
        "id": "2TbhA3Gd9-kV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagging, also known as Bootstrap aggregating, is an ensemble learning technique that helps to improve the performance and accuracy of machine learning algorithms. It is used to deal with bias-variance trade-offs and reduces the variance of a prediction model. Bagging avoids overfitting of data and is used for both regression and classification models, specifically for decision tree algorithms.\n",
        "\n"
      ],
      "metadata": {
        "id": "dxAi9IJTByVD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WXn6tWsF-Edy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. What is boosting?**"
      ],
      "metadata": {
        "id": "x1hIGiAp-E64"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boosting is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers. It is done by building a model by using weak models in series. Firstly, a model is built from the training data. Then the second model is built which tries to correct the errors present in the first model. This procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models are added. "
      ],
      "metadata": {
        "id": "N_Xgg_KsCURh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gdw1ucXb-PgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. What are the benefits of using ensemble techniques?**"
      ],
      "metadata": {
        "id": "nRGy_fQd-P3u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ensemble techniques are machine learning methods that combine multiple models to improve the accuracy of predictions . The benefits of using ensemble techniques include:**\n",
        "\n",
        "- Reducing variance: Ensemble techniques can reduce the spread in the average skill of a predictive model \n",
        "- Improving prediction performance: Ensemble techniques can improve the average prediction performance over any contributing member in the ensemble.\n",
        "- Increasing accuracy: Ensemble methods are ideal for reducing the variance in models, thereby increasing the accuracy of predictions "
      ],
      "metadata": {
        "id": "rZdB-tSoC1-n"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "44Ro7IB9-R9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6. Are ensemble techniques always better than individual models?**"
      ],
      "metadata": {
        "id": "PaROvRSr-SRm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no absolute guarantee a ensemble model performs better than an individual model, but if you build many of those, and your individual classifier is weak. Your overall performance should be better than an individual model. In machine learning, training multiple models generally outperform training a single model.\n"
      ],
      "metadata": {
        "id": "k6py8YXvDyY8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z76i4_Hr-W5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7. How is the confidence interval calculated using bootstrap?**"
      ],
      "metadata": {
        "id": "t07kHG7w-XRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bootstrapping confidence interval is a statistical method for inference that does not make strong distributional assumptions. It can be used to estimate the confidence interval (CI) for statistics that have no closed-form or complicated solutions. It involves drawing samples with replacement from sample data and computing the statistic of interest for each sample. The CI is then obtained by finding the middle percentage of the statistics from the samples"
      ],
      "metadata": {
        "id": "0ECxqMwtETL9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IVxOE-ET-eRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8. How does bootstrap work and What are the steps involved in bootstrap?**"
      ],
      "metadata": {
        "id": "q_oCA1EN-ept"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bootstrap confidence interval is a statistical method for inference that does not make strong distributional assumptions. It can be used to estimate the confidence interval (CI) for statistics that have no closed-form or complicated solutions. It involves drawing samples with replacement from sample data and computing the statistic of interest for each sample. The CI is then obtained by finding the middle percentage of the statistics from the samples. "
      ],
      "metadata": {
        "id": "OMnsRgwbE-6O"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dBQ2NnTL-xzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a\n",
        "sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use\n",
        "bootstrap to estimate the 95% confidence interval for the population mean height.**"
      ],
      "metadata": {
        "id": "I2RoW5-5-yDv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To estimate the 95% confidence interval for the population mean height using bootstrap, we can follow these steps:\n",
        "\n",
        "- Take repeated samples (with replacement) from the original sample of 50 trees.\n",
        "- Calculate the mean height for each bootstrap sample.\n",
        "- Repeat steps 1 and 2 many times (e.g., 10,000 times).\n",
        "- Calculate the 2.5th and 97.5th percentiles of the bootstrap distribution of means.\n",
        "- The resulting interval is the estimated 95% confidence interval for the population mean height.\n",
        "- Using this method, we can estimate that the 95% confidence interval for the population mean height is approximately (14.3 meters, 15.7 meters) ."
      ],
      "metadata": {
        "id": "3gekeD7vM8yZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "\n",
        "# define dataset\n",
        "data = [15] * 50\n",
        "\n",
        "# configure bootstrap\n",
        "n_iterations = 1000\n",
        "n_size = int(len(data))\n",
        "\n",
        "# run bootstrap\n",
        "stats = list()\n",
        "for i in range(n_iterations):\n",
        "    # prepare train and test sets\n",
        "    sample = resample(data, n_samples=n_size)\n",
        "    # calculate statistic\n",
        "    stat = mean(sample)\n",
        "    # store bootstrap sample\n",
        "    stats.append(stat)\n",
        "\n",
        "# calculate 95% confidence intervals (1.96 standard deviations from mean)\n",
        "lower = mean(stats) - 1.96 * std(stats)\n",
        "upper = mean(stats) + 1.96 * std(stats)\n",
        "\n",
        "print(lower, upper)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PifKadFw_BUk",
        "outputId": "a267a8a7-55b5-4c4c-9a6b-8002b4c75021"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15.0 15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bj7g2hzvNmV5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}