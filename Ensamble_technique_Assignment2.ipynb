{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcgO6XDr7S0xQZZHFlILQ5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riturajkumari/Ensemble_technique/blob/main/Ensamble_technique_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. How does bagging reduce overfitting in decision trees?**"
      ],
      "metadata": {
        "id": "VWlZY8A99G7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Bagging is a technique used to reduce overfitting in decision trees. It works by creating multiple subsets of the training data set and training each subset on a separate decision tree.\n",
        "**bagging reduce overfitting in decision trees**\n",
        "-  Bagging is an ensemble technique that uses bootstrap resampling to generate multiple different subsets of the training data, and then trains a separate model on each subset. The Bagging Classifier can be used to improve the performance of any base classifier that has high variance, it reduces the variance of the model and can help to reduce overfitting. The Bagging classifier is a general-purpose ensemble method that can be used with a variety of different base models."
      ],
      "metadata": {
        "id": "6QHKQbZq-W-N"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rdZj6CQC9TNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. What are the advantages and disadvantages of using different types of base learners in bagging?**"
      ],
      "metadata": {
        "id": "pS4VEXEo9TZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**advantages**\n",
        "- The biggest advantage of bagging is that multiple weak learners can work better than a single strong learner.\n",
        "- It provides stability and increases the machine learning algorithm’s accuracy - that is used in statistical classification and regression.\n",
        "- It helps in reducing variance, i.e. it avoids overfitting."
      ],
      "metadata": {
        "id": "4Pk9aA8KASKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Disadvantage**\n",
        "\n",
        "it may result in high bias if it is not modelled properly and thus may result in underfitting.\n",
        "Since we must use multiple models, it becomes computationally expensive and may not be suitable in various use cases."
      ],
      "metadata": {
        "id": "DZ0MZ7OpAfH0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0CEOWzNm9XKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. How does the choice of base learner affect the bias-variance tradeoff in bagging?**"
      ],
      "metadata": {
        "id": "E2rNKPxM9XVI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The equation ε = [g(X) − f(X′)] + δ or error = bias + variance represents the bias variance tradeoff for some model . e error fixed, decreasing the bias means then the model variance increases. \n",
        "Likewise, decreasing the model variance means that the bias must increase.\n",
        "- Bias refers to the error that is introduced by approximating a real-life problem with a simplified model. A model with high bias is not able to capture the true complexity of the data and tends to underfit, leading to poor performance on both the training and test data. The bias is represented by the difference between the expected or true value of the target variable and the predicted value of the model.\n",
        "- Variance refers to the error introduced by the model’s sensitivity to small fluctuations in the training data. A model with high variance tends to overfit the training data, leading to poor performance on new, unseen data. Variance is represented by the degree of variability or spreads in the model’s predictions for different training sets."
      ],
      "metadata": {
        "id": "4NmNvoiHCLxC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hvgf9aFl9bFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. Can bagging be used for both classification and regression tasks? How does it differ in each case?**"
      ],
      "metadata": {
        "id": "Dmg-eUmP9bRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Yes, bagging can be used for both classification and regression tasks. Bagging is a technique that is used to reduce the variance of a prediction model and avoid overfitting of data . It is used to deal with bias-variance trade-offs and is specifically used for decision tree algorithms ."
      ],
      "metadata": {
        "id": "yWgyVNbMFqZH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ejNM3TFc9fGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. What is the role of ensemble size in bagging? How many models should be included in the ensemble?**"
      ],
      "metadata": {
        "id": "FSv3r5q89fSH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  In the case of Bagging, every element has the same probability to appear in a new dataset. By increasing the size of the training set, the model’s predictive force can’t be improved. It decreases the variance and narrowly tunes the prediction to an expected outcome.\n",
        "\n",
        "These multisets of data are used to train multiple models. \n",
        "\n",
        "- There are no restrictions/guidelines on the number of models. You can keep the number of models as a hyperparameter if the training cost is less."
      ],
      "metadata": {
        "id": "H4lv7WeQGx6-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VK0K01h89jDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6. Can you provide an example of a real-world application of bagging in machine learning?**"
      ],
      "metadata": {
        "id": "tdDe-DAE9jNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Example of Bagging The Random Forest model uses Bagging, where decision tree models with higher variance are present. It makes random feature selection to grow trees. Several random trees make a Random Forest.\n",
        "- Different training data subsets are selected using row sampling with replacement and random sampling methods from the entire training dataset.\n",
        "- Bagging tries to solve the over-fitting problem.\n",
        "- If the classifier is unstable (high variance), then apply bagging.\n",
        "- In this base classifiers are trained parallelly."
      ],
      "metadata": {
        "id": "nqAQb50oHjGL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "adxZNoo4-HjN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}