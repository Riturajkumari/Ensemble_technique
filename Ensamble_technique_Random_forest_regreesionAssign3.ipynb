{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKb4/22hw3AyzuGbSpH6uf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riturajkumari/Ensemble_technique/blob/main/Ensamble_technique_Random_forest_regreesionAssign3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. What is Random Forest Regressor?**"
      ],
      "metadata": {
        "id": "pcYaeAHyWkDI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Random Forest is an ensemble learning technique used for both classification and regression problems. In this technique, multiple decision trees are created and their output is averaged to give the final result. Random Forest Regression is known to produce very robust results by avoiding overfitting."
      ],
      "metadata": {
        "id": "mfbkHhasX2-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Random forest regression is a supervised learning algorithm that uses an ensemble learning method for regression.\n",
        "\n",
        "Random forest is a bagging technique and not a boosting technique. The trees in random forests run in parallel, meaning there is no interaction between these trees while building the trees."
      ],
      "metadata": {
        "id": "n4EnGC8MZF8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kz7XsKM_WlJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oGLKXOLyWlnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. How does Random Forest Regressor reduce the risk of overfitting?**"
      ],
      "metadata": {
        "id": "yT9Igg7xWlyQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forests deals with the problem of overfitting by creating multiple trees, with each tree trained slightly differently so it overfits differently. Random forests is a classifier that combines a large number of decision trees. The decisions of each tree are then combined to make the final classification."
      ],
      "metadata": {
        "id": "PQ3XmTuOYsw7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9K3GJoKNWo05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. How does Random Forest Regressor aggregate the predictions of multiple decision trees?**"
      ],
      "metadata": {
        "id": "px91-y9_Wo_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest operates by constructing a multitude of decision trees at training time and outputting the clas s that’s the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
        "\n",
        "A random forest is a meta-estimator (i.e. it combines the result of multiple predictions), which aggregates many decision trees with some helpful modifications:\n",
        "- The number of features that can be split at each node is limited to some percentage of the total (which is known as the hyper-parameter). \n",
        "This limitation ensures that the ensemble model does not rely too heavily on any individual feature and makes fair use of all potentially predictive features.\n",
        "- Each tree draws a random sample from the original data set when generating its splits, adding a further element of randomness that prevents overfitting. "
      ],
      "metadata": {
        "id": "FZZBvDE8ZU4m"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qE6y2uY3WsEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. What are the hyperparameters of Random Forest Regressor?**"
      ],
      "metadata": {
        "id": "eGb_m4UUWsOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important Hyperparameters in Random Forest**\n",
        "Hyperparameters are used in random forests to either enhance the performance and predictive power of models or to make the model faster.\n",
        "\n",
        "**Hyperparameters to Increase the Predictive Power**\n",
        "- n_estimators: Number of trees the algorithm builds before averaging the predictions.\n",
        "\n",
        "- max_features: Maximum number of features random forest considers splitting a node.\n",
        "\n",
        "- mini_sample_leaf: Determines the minimum number of leaves required to split an internal node.\n",
        "\n",
        "- criterion: How to split the node in each tree? (Entropy/Gini impurity/Log Loss)\n",
        "\n",
        "- max_leaf_nodes: Maximum leaf nodes in each tree"
      ],
      "metadata": {
        "id": "SwvOc1JgdP21"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G3lSxre0WvK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameters to Increase the Speed**\n",
        "- n_jobs: it tells the engine how many processors it is allowed to use. If the value is 1, it can use only one processor, but if the value is -1, there is no limit.\n",
        "\n",
        "- random_state: controls randomness of the sample. The model will always produce the same results if it has a definite value of random state and has been given the same hyperparameters and training data.\n",
        "\n",
        "- oob_score: OOB means out of the bag. It is a random forest cross-validation method. In this, one-third of the sample is not used to train the data; instead used to evaluate its performance. These samples are called out-of-bag samples."
      ],
      "metadata": {
        "id": "ikTdeLJ7djwx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3rDEM2-bWvtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. What is the difference between Random Forest Regressor and Decision Tree Regressor?**"
      ],
      "metadata": {
        "id": "UiYKNiRmWv3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest Regressor**\n",
        "- Decision trees normally suffer from the problem of overfitting if it’s allowed to grow without any control.\n",
        "- A single decision tree is faster in computation.\n",
        "- When a data set with features is taken as input by a decision tree, it will formulate some rules to make predictions.\n",
        "\n",
        "**Random Forest Regressor**\n",
        "- Random forests are created from subsets of data, and the final output is based on average or majority ranking; hence the problem of overfitting is taken care of.\n",
        "- It is comparatively slower.\n",
        "- Random forest randomly selects observations, builds a decision tree, and takes the average result. It doesn’t use any set of formulas."
      ],
      "metadata": {
        "id": "cRogbIUReqhL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KRhFS6M3Wyls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zqGmyLFPWzTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6. What are the advantages and disadvantages of Random Forest Regressor?**"
      ],
      "metadata": {
        "id": "vYDwl5aJWzej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advantages of Random Forest Regressor**\n",
        "- Random forest is one of the most accurate learning algorithms available. For many data sets, it produces a highly accurate classifier.\n",
        "- It runs efficiently on large databases.\n",
        "- It can handle thousands of input variables without variable deletion.\n",
        "- It gives estimates of what variables are important in the classification.\n",
        "- It generates an internal unbiased estimate of the generalization error as the forest building progresses.\n",
        "- It has an effective method for estimating missing data and maintains accuracy when a large proportion of the data are missing."
      ],
      "metadata": {
        "id": "YIKSHgEqZpGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**disadvantages of Random Forest Regressor**\n",
        "- Random forests have been observed to overfit for some data sets with noisy classification/regression tasks.\n",
        "- For data including categorical variables with different numbers of levels, random forests are biased in favor of those attributes with more levels. Therefore, the variable importance scores from random forest are not reliable for this type of data."
      ],
      "metadata": {
        "id": "1X4mFJ5VZ6tc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_BlSDgm-W2xJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7. What is the output of Random Forest Regressor?**"
      ],
      "metadata": {
        "id": "H1Ue74zAW5zp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Average prediction across estimators. Each decision tree regression predicts a number as an output for a given input. Random forest regression takes the average of those predictions as its ‘final’ output."
      ],
      "metadata": {
        "id": "EAFFbcgsfzOp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AAHLkBmWf9Ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8. Can Random Forest Regressor be used for classification tasks?**"
      ],
      "metadata": {
        "id": "5bloK8-RW8Hb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Yes Random Forest Regressor be used for classification task.Random Forest is one of the popular machine-learning algorithms which can be used in classification and regression tasks.\n",
        "\n",
        "- The Random Forest algorithm creates many decision trees (a forest) and takes the majority vote out of all the decision trees if it is a classification problem. If it is a regression problem, the mean of all decision tree outputs is taken as the final result. "
      ],
      "metadata": {
        "id": "TA8HVwdVg6Mm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KIo5zOWPW_Zd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}