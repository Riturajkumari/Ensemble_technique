{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOx1pmHZTOg0RlMjrmRSQUI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riturajkumari/Ensemble_technique/blob/main/Boosting_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. What is boosting in machine learning?**"
      ],
      "metadata": {
        "id": "lUkY4wakuZuX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Boosting** is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers. It is done by building a model by using weak models in series. Firstly, a model is built from the training data. Then the second model is built which tries to correct the errors present in the first model. This procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models are added."
      ],
      "metadata": {
        "id": "DCyMHBCiwN0a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s4zNaKumubqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. What are the advantages and limitations of using boosting techniques?**"
      ],
      "metadata": {
        "id": "xGYbwM11ub4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advantages of Boosting**\n",
        "- Improved Accuracy – Boosting can improve the accuracy of the model by combining several weak models’ accuracies and averaging them for regression or voting over them for classification to increase the accuracy of the final model.\n",
        "- Robustness to Overfitting – Boosting can reduce the risk of overfitting by reweighting the inputs that are classified wrongly.\n",
        "- Better handling of imbalanced data – Boosting can handle the imbalance data by focusing more on the data points that are misclassified\n",
        "- Better Interpretability – Boosting can increase the interpretability of the model by breaking the model decision process into multiple processes."
      ],
      "metadata": {
        "id": "TZoTn3FezRr2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Pk3xnwDujux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. Explain how boosting works.**"
      ],
      "metadata": {
        "id": "NqTBMa3tuj9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- base learning (ML) algorithms with a different distribution. Each time base learning algorithm is applied, it generates a new weak prediction rule. This is an iterative process. After many iterations, the boosting algorithm combines these weak rules into a single strong prediction rule.\n",
        "\n",
        "- here are the following steps:\n",
        "\n",
        "Step 1:  The base learner takes all the distributions and assign equal weight or attention to each observation.\n",
        "\n",
        "Step 2: If there is any prediction error caused by first base learning algorithm, then we pay higher attention to observations having prediction error. Then, we apply the next base learning algorithm.\n",
        "\n",
        "Step 3: Iterate Step 2 till the limit of base learning algorithm is reached or higher accuracy is achieved."
      ],
      "metadata": {
        "id": "3zByDOD90K81"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RZkksY4_umBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. What are the different types of boosting algorithms?**"
      ],
      "metadata": {
        "id": "ipmo6PQJumOD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are different types of boosting algorithms used in machine learning. Some of the popular ones are:\n",
        "\n",
        "- AdaBoost (Adaptive Boosting) algorithm: AdaBoost is a boosting algorithm that combines multiple weak classifiers to create a strong classifier. It is used for classification problems and is one of the most popular boosting algorithms.\n",
        "\n",
        "- Gradient Boosting algorithm: Gradient Boosting is another popular boosting algorithm that is used for regression and classification problems. It works by combining multiple weak models to create a strong model.\n",
        "\n",
        "- XG Boost algorithm: XG Boost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It is used for regression, classification, and ranking problems."
      ],
      "metadata": {
        "id": "QpwFaZsk0qTX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IE083cVcupzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. What are some common parameters in boosting algorithms?**"
      ],
      "metadata": {
        "id": "JRjLadQ2uqDU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some common parameters in boosting algorithms are:\n",
        "\n",
        "- n_estimators: The number of trees in the forest.\n",
        "- learning_rate: The rate at which the algorithm learns.\n",
        "- max_depth: The maximum depth of each tree in the forest.\n",
        "- base_estimator: The type of estimator to use as a base for boosting."
      ],
      "metadata": {
        "id": "b3EsFtqz_nQt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2bogqIxzuuO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6. How do boosting algorithms combine weak learners to create a strong learner?**"
      ],
      "metadata": {
        "id": "ONrf5Pvnuubh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Boosting is an ensemble learning method that combines a set of weak learners into a strong learner to minimize training errors.\n",
        "- In boosting, a random sample of data is selected, fitted with a model and then trained sequentially—that is, each model tries to compensate for the weaknesses of its predecessor. With each iteration, the weak rules from each individual classifier are combined to form one, strong prediction rule.\n"
      ],
      "metadata": {
        "id": "oG0WO7d2Bgxc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nJCByrUYuyIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7. Explain the concept of AdaBoost algorithm and its working.**"
      ],
      "metadata": {
        "id": "NIG1SANtuyW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- AdaBoost is a machine learning algorithm that is used for classification and regression problems. It is an ensemble method that combines multiple weak learners to create a strong learner. The algorithm works by iteratively training the model by selecting the training set based on the accurate prediction of the last training.\n",
        "-  Initially, AdaBoost selects a training subset randomly. Then, it trains the AdaBoost machine learning model by selecting the training set based on the accurate prediction of the last training. The algorithm puts more weight on difficult-to-classify instances and less on those already handled well. The final model is a weighted sum of all the weak learners."
      ],
      "metadata": {
        "id": "kBWGgn1mDPXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AdaBoost works in five steps:\n",
        "\n",
        "- Initially, Adaboost selects a training subset randomly.\n",
        "- It iteratively trains the AdaBoost machine learning model by selecting the - - training set based on the accurate prediction of the last training.\n",
        "- It assigns higher weights to misclassified instances.\n",
        "- It trains subsequent models on the updated weights.\n",
        "- Finally, it combines all models via weighted majority voting (or summing) to produce the final prediction."
      ],
      "metadata": {
        "id": "qAM6freiDoSy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rGd4ODcgu2la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8. What is the loss function used in AdaBoost algorithm?**"
      ],
      "metadata": {
        "id": "pimlRNjIu2wR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The loss function used in AdaBoost algorithm is the exponential loss function.\n",
        "-  The exponential loss function is used to construct the Adaboost loss.\n",
        "-  The Adaboost algorithm is equivalent to forward stagewise additive modeling using an exponential loss function.\n",
        "- The Adaboost algorithm is a boosting algorithm that combines multiple weak classifiers to form a strong classifier. The weak classifiers are trained on different subsets of the training data."
      ],
      "metadata": {
        "id": "q1gz8o8KEJJc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X07ItbChu6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9. How does the AdaBoost algorithm update the weights of misclassified samples?**"
      ],
      "metadata": {
        "id": "Wm-AyvKmu6TH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The Adaboost Algorithm works by iteratively training a series of weak classifiers on weighted versions of the training data.\n",
        "- The basic idea of AdaBoost is to increase the weights of misclassified samples and reduce the weights of correctly classified samples.\n",
        "-  For solving the problem of unlimited increase of noisy samples’ weights during the iteration process, Sun et al. proposed a new AdaBoost algorithm based on Cao’s algorithm."
      ],
      "metadata": {
        "id": "lfVY7qWEE7vd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ucm2MRqPvAZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?**"
      ],
      "metadata": {
        "id": "WiBu91kKvAxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In AdaBoost algorithm, increasing the number of estimators can lead to overfitting and worse performance.\n",
        "- hyperparameter for Adaboost is n_estimator. Often by changing the number of base models or weak learners we can adjust the accuracy of the model. The number of trees added to the model must be high for the model to work well, often hundreds, if not thousands. Afterall the more is the number of weak learners, the more the model will change from being high biased to low biased."
      ],
      "metadata": {
        "id": "2umlDi2pH7gz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SYu8j15UvFR7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}